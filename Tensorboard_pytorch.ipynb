{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QZX7rfVDSyD",
        "outputId": "2f5325af-cd4d-4c78-c322-03b7547599d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.56.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.41.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision matplotlib tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_F6hWgHSDkVH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kantip/anaconda3/envs/python38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Introduction\n",
        "In this notebook, we’ll be training a variant of LeNet-5 against the Fashion-MNIST dataset.\n",
        "Fashion-MNIST is a set of image tiles depicting various garments,\n",
        "with ten class labels indicating the type of garment depicted.\n",
        "\"\"\"\n",
        "\n",
        "# Pytorch model and training necessities\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Image datasets and image manipulation\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Pytorch TensorBoard support\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# In case you are using an environment that has TensorFlow installed,\n",
        "# such as Google Colab, uncomment the following code to avoid\n",
        "# a bug with saving embeddings to your TensorBoard directory\n",
        "\n",
        "# import tensorflow as tf\n",
        "# import tensorboard as tb\n",
        "# tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "zvA_tolbEuQh",
        "outputId": "2ca25601-5cea-4b07-9800-b7af8960d1a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26422272it [00:14, 1881042.34it/s]                              \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "29696it [00:00, 144724.95it/s]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4422656it [00:02, 1812999.33it/s]                             \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6144it [00:00, 12025106.76it/s]         "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Showing Images in TensorBoard\n",
        "# Gather datasets and prepare them for consumption\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,),(0.5,))\n",
        "])\n",
        "\n",
        "# Store separate traning and validating splits in ./data\n",
        "training_set = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\",\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "validation_set = torchvision.datasets.FashionMNIST(\n",
        "    root='./data',\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    training_set,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    validation_set,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# Class labels\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "# Helper function for inline image display\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# Extract a batch of 4 images\n",
        "dataiter = iter(training_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Create grid from the images and show them\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "matplotlib_imshow(img_grid, one_channel=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "YZads2soOuFo",
        "outputId": "0b65cc31-2370-42a2-9166-23ac2aef971d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nIf you start TensorBoard at the command line and open it in a new browser tab\\n(usually at localhost:6006),\\nyou should see the image grid under the IMAGES tab.\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Above, we used TorchVision and Matplotlib to create a visual grid of a minibatch of our input data.\n",
        "Below, we use the add_image() call on SummaryWriter to log the image for consumption by TensorBoard,\n",
        "and we also call flush() to make sure it’s written to disk right away.\n",
        "\"\"\"\n",
        "\n",
        "# Default log_dir argument is \"runs\" - but it's goood to be specific\n",
        "# torch.utils.tensorboard.SummaryWriter is imported above\n",
        "writer = SummaryWriter(\"runs/fashion_mnist_experiment_1\")\n",
        "\n",
        "# Write image data to TensorBoard log dir\n",
        "writer.add_image(F\"Four Fashion-MNIST Images\", img_grid)\n",
        "writer.flush()\n",
        "\n",
        "# To view, start TensorBoard on the command line with:\n",
        "#   tensorboard --logdir=runs\n",
        "# ...and open a browser tab to http://localhost:6006/\n",
        "\n",
        "\"\"\"\n",
        "If you start TensorBoard at the command line and open it in a new browser tab\n",
        "(usually at localhost:6006),\n",
        "you should see the image grid under the IMAGES tab.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Nlr7UrQkJw"
      },
      "source": [
        "***Graphing Scalars to Visualize Training***\n",
        "* TensorBoard is useful for tracking the progress and efficacy of your training. Below, we’ll run a training loop, track some metrics, and save the data for TensorBoard’s consumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vGPW0gsmQM4E"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,6,5)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv2 = nn.Conv2d(6,16,5)\n",
        "        self.fc1 = nn.Linear(16*4*4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16*4*4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(),lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS-XrNO1S8v0",
        "outputId": "55905738-e7fd-4ee3-cae5-0ba5f56b20b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Number of validation loader : 2500\n",
            "Batch 1000\n",
            "train loss: 0.3327255552636925, val loss: 0.34545030004991567\n",
            "Batch 2000\n",
            "train loss: 0.33344277851693915, val loss: 0.35100915413549955\n",
            "Batch 3000\n",
            "train loss: 0.3162693905665219, val loss: 0.37156633878568135\n",
            "Batch 4000\n",
            "train loss: 0.3206511352878861, val loss: 0.35611339125766245\n",
            "Batch 5000\n",
            "train loss: 0.3147794957934093, val loss: 0.3413432437701238\n",
            "Batch 6000\n",
            "train loss: 0.35560640785598663, val loss: 0.33323818389862425\n",
            "Batch 7000\n",
            "train loss: 0.312713975404331, val loss: 0.3452357994686972\n",
            "Batch 8000\n",
            "train loss: 0.29010669457487165, val loss: 0.35976056240680626\n",
            "Batch 9000\n",
            "train loss: 0.31463613757066194, val loss: 0.35371846483540703\n",
            "Batch 10000\n",
            "train loss: 0.33633319627918534, val loss: 0.3365860953689109\n",
            "Batch 11000\n",
            "train loss: 0.3233412541206053, val loss: 0.3498185844729931\n",
            "Batch 12000\n",
            "train loss: 0.3010631700804024, val loss: 0.3274451821153165\n",
            "Batch 13000\n",
            "train loss: 0.3035205461163423, val loss: 0.3350903860272063\n",
            "Batch 14000\n",
            "train loss: 0.3109990097176924, val loss: 0.338416677092154\n",
            "Batch 15000\n",
            "train loss: 0.30825152126267497, val loss: 0.3330249742240052\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Now let's train a single epoch, and evaluate the training vs validation set losses every 1000 batches:\n",
        "print(f\"--- Number of validation loader : {len(validation_loader)}\")\n",
        "for epoch in range(1):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(training_loader, 0):\n",
        "        #basic training loop\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            print(\"Batch {}\".format(i+1))\n",
        "            # Check against the validation set\n",
        "            running_vloss = 0.0\n",
        "\n",
        "            # net.train(False) # Don't need to track gradients for validation\n",
        "            net.eval()\n",
        "            for j, vdata in enumerate(validation_loader, 0):\n",
        "                vinputs, vlabels = vdata\n",
        "                voutputs = net(vinputs)\n",
        "                vloss = criterion(voutputs, vlabels)\n",
        "                running_vloss += vloss.item()\n",
        "            net.train(True)\n",
        "\n",
        "            avg_loss = running_loss / 1000\n",
        "            avg_vloss = running_vloss / len(validation_loader)\n",
        "\n",
        "            # Log the running loss averaged per batch\n",
        "            writer.add_scalars(\"Training vs. Validation Loss\",\n",
        "                               {\"Training\": avg_loss, \"Validation\": avg_vloss},\n",
        "                               epoch * len(training_loader) + i)\n",
        "\n",
        "            print(f\"train loss: {avg_loss}, val loss: {avg_vloss}\")\n",
        "            running_loss=0\n",
        "print(\"Finished Training\")\n",
        "\n",
        "writer.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lCFnGivAOpb"
      },
      "source": [
        "***Visualizin Your Model***\n",
        "* TensorBoard can also be used to examine the data flow within your model. To do this, call the add_graph() method with a model and sample input. When you open"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lsk_wNVWAT2_"
      },
      "outputs": [],
      "source": [
        "# Again, grab a single mini-batch of images\n",
        "dataiter = iter(training_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# add_graph() will trace the sample input through, your model\n",
        "# and render it as a graph.\n",
        "writer.add_graph(net, images)\n",
        "writer.flush()\n",
        "\n",
        "# When you switch over to TensorBoard, you should see a GRAPHS tab.\n",
        "# Double-click the “NET” node to see the layers and data flow within your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al7nzKVaI0b-"
      },
      "source": [
        "***Visualizing Your Dataset with Embeddings***\n",
        "* The 28-by-28 image tiles we’re using can be modeled as 784-dimensional vectors (28 * 28 = 784). It can be instructive to project this to a lower-dimensional representation. The add_embedding() method will project a set of data onto the three dimensions with highest variance, and display them as an interactive 3D chart. The add_embedding() method does this automatically by projecting to the three dimensions with highest variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hfzeW3COI8xe"
      },
      "outputs": [],
      "source": [
        "# Select a random subset of data and corresponding labels\n",
        "def select_n_random(data, labels, n=100):\n",
        "    assert len(data) == len(labels)\n",
        "\n",
        "    perm = torch.randperm(len(data))\n",
        "    return data[perm][:n], labels[perm][:n]\n",
        "\n",
        "# Extract a random subset of data\n",
        "images, labels = select_n_random(training_set.data, training_set.targets)\n",
        "\n",
        "# get the class labels for each image\n",
        "class_labels = [classes[label] for label in labels]\n",
        "\n",
        "# log embeddings\n",
        "features = images.view(-1, 28 * 28)\n",
        "writer.add_embedding(features,\n",
        "                    metadata=class_labels,\n",
        "                    label_img=images.unsqueeze(1))\n",
        "writer.flush()\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
